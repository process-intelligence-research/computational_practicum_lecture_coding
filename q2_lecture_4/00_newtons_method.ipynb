{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this live coding exercise, we will implement Newton's method for unconstrained optimization.\\\n",
    "We will then use the implemented method to try and find the minimum of Rosenbrock function expressed as:\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (x_i - 1)^2\\right]\n",
    "$$\\\n",
    "Rosenbrock function is a standard test function used for testing gradient-descent based algorithms.\\\n",
    "It can be defined in $n$-dimensions arbitrarily. It is unimodal and the global minimum of the function is 0 at $x_1, \\ldots, x_n = 1$.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2-D Rosenbrock function is expressed as:\n",
    "$$\n",
    "f(x, y) = (1 - x)^2 + 100(y - x^2)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Python function implementing Newton's method\n",
    "def newtons_method(grad_f, hessian, x0, epsilon=1e-6, k_max=100):\n",
    "    \"\"\"\n",
    "    Implements Newton's method for optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    grad_f (function): A function that computes the gradient of f at a point x.\n",
    "    hessian (function): A function that computes the Hessian of f at a point x.\n",
    "    x0 (numpy.ndarray): Initial guess for the solution.\n",
    "    epsilon (float): Tolerance for stopping criteria (default: 1e-6).\n",
    "    k_max (int): Maximum number of iterations (default: 100).\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: The final value of x that minimizes the function.\n",
    "    \"\"\"\n",
    "    # Initialize variables\n",
    "    x = x0\n",
    "    k = 1\n",
    "    delta = np.full_like(x0, np.inf)  # Initialize delta as large values\n",
    "\n",
    "    # Main optimization loop\n",
    "    while np.linalg.norm(delta) > epsilon and k <= k_max:\n",
    "        # Compute the Newton step\n",
    "        grad = grad_f(x)  # Gradient at current x\n",
    "        hess = hessian(x)  # Hessian at current x\n",
    "        \n",
    "        # Solve for the Newton step Δ: H(x) * Δ = grad_f(x)\n",
    "        delta = np.linalg.solve(hess, grad)\n",
    "        \n",
    "        # Update x\n",
    "        x = x - delta\n",
    "        \n",
    "        # Increment iteration counter\n",
    "        k += 1\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient of $f(x, y)$\n",
    "The gradient $\\nabla f(x, y)$ is the vector of partial derivatives of $f(x, y)$ with respect to $x$ and $y$:\n",
    "$$\n",
    "\\nabla f(x, y) = \\begin{bmatrix} \n",
    "\\frac{\\partial f}{\\partial x} \\\\\n",
    "\\frac{\\partial f}{\\partial y}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "##### Partial Derivative with Respect to $x$:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ (1 - x)^2 + 100(y - x^2)^2 \\right]\n",
    "$$\n",
    "\n",
    "- First term: $(1 - x)^2$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}(1 - x)^2 = -2(1 - x)\n",
    "$$\n",
    "\n",
    "- Second term: $100(y - x^2)^2$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x} 100(y - x^2)^2 = 100 \\cdot 2(y - x^2) \\cdot (-2x) = -400x(y - x^2)\n",
    "$$\n",
    "\n",
    "Combining both terms:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = -2(1 - x) - 400x(y - x^2)\n",
    "$$\n",
    "\n",
    "##### Partial Derivative with Respect to $y$:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} \\left[ (1 - x)^2 + 100(y - x^2)^2 \\right]\n",
    "$$\n",
    "\n",
    "- First term: $(1 - x)^2$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y}(1 - x)^2 = 0 \\quad (\\text{no dependence on } y)\n",
    "$$\n",
    "\n",
    "- Second term: $100(y - x^2)^2$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y} 100(y - x^2)^2 = 100 \\cdot 2(y - x^2) = 200(y - x^2)\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = 200(y - x^2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Gradient Expression:\n",
    "$$\n",
    "\\nabla f(x, y) = \\begin{bmatrix}\n",
    "-2(1 - x) - 400x(y - x^2) \\\\\n",
    "200(y - x^2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This gradient is used in the Newton's method implementation to compute the Newton step $\\Delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rosenbrock function's gradient\n",
    "def grad_rosenbrock(x):\n",
    "    \"\"\"\n",
    "    Gradient of the 2D Rosenbrock function.\n",
    "    \"\"\"\n",
    "    x1, x2 = x\n",
    "    grad_x1 = -2 * (1 - x1) - 400 * x1 * (x2 - x1**2)\n",
    "    grad_x2 = 200 * (x2 - x1**2)\n",
    "    return np.array([grad_x1, grad_x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second Partial Derivative with Respect to $x$:\n",
    "The second derivative $\\frac{\\partial^2 f}{\\partial x^2}$ is:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left[ -2(1 - x) - 400x(y - x^2) \\right]\n",
    "$$\n",
    "\n",
    "- First term: $-2(1 - x)$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x}[-2(1 - x)] = 2\n",
    "$$\n",
    "\n",
    "- Second term: $-400x(y - x^2)$:\n",
    "  - First, expand $\\frac{\\partial}{\\partial x}[-400x(y - x^2)]$:\n",
    "  $$ \n",
    "  \\frac{\\partial}{\\partial x}[-400x(y - x^2)] = \\frac{\\partial}{\\partial x}[-400xy + 400x^3] \n",
    "  $$\n",
    "  - For $-400xy$: \n",
    "  $$\n",
    "  \\frac{\\partial}{\\partial x}[-400xy] = -400y\n",
    "  $$\n",
    "  - For $400x^3$: \n",
    "  $$\n",
    "  \\frac{\\partial}{\\partial x}[400x^3] = 1200x^2\n",
    "  $$\n",
    "\n",
    "Combining the terms:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x^2} = 2 - 400y + 1200x^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Mixed Partial Derivative $\\frac{\\partial^2 f}{\\partial x \\partial y}$:\n",
    "The mixed partial derivative $\\frac{\\partial^2 f}{\\partial x \\partial y}$ is:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y} \\left[ -2(1 - x) - 400x(y - x^2) \\right]\n",
    "$$\n",
    "\n",
    "- First term: $-2(1 - x)$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y}[-2(1 - x)] = 0 \\quad (\\text{no dependence on } y)\n",
    "$$\n",
    "\n",
    "- Second term: $-400x(y - x^2)$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y}[-400x(y - x^2)] = -400x\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial x \\partial y} = -400x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Second Partial Derivative with Respect to $y$:\n",
    "The second derivative $\\frac{\\partial^2 f}{\\partial y^2}$ is:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y^2} = \\frac{\\partial}{\\partial y} \\left[ 200(y - x^2) \\right]\n",
    "$$\n",
    "\n",
    "- First term: $200(y - x^2)$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y}[200(y - x^2)] = 200\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial y^2} = 200\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Hessian Matrix:\n",
    "Combining all terms, the Hessian matrix $\\mathbf{H}$ is:\n",
    "$$\n",
    "\\mathbf{H} = \n",
    "\\begin{bmatrix}\n",
    "2 - 400y + 1200x^2 & -400x \\\\\n",
    "-400x & 200\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This matrix is used in Newton's method to compute the Newton step $\\Delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rosenbrock function's Hessian\n",
    "def hessian_rosenbrock(x):\n",
    "    \"\"\"\n",
    "    Hessian of the 2D Rosenbrock function.\n",
    "    \"\"\"\n",
    "    x1, x2 = x\n",
    "    h11 = 2 - 400 * (x2 - x1**2) + 800 * x1**2\n",
    "    h12 = -400 * x1\n",
    "    h21 = -400 * x1\n",
    "    h22 = 200\n",
    "    return np.array([[h11, h12], [h21, h22]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us provide the guess as $x=0$ and $y=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the Newton's method we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkaria\\OneDrive - Delft University of Technology\\Teaching\\CH3133-CP\\computational_practicum_lecture_coding\\.venv\\Lib\\site-packages\\numpy\\_core\\numeric.py:457: RuntimeWarning: invalid value encountered in cast\n",
      "  multiarray.copyto(res, fill_value, casting='unsafe')\n"
     ]
    }
   ],
   "source": [
    "result = newtons_method(grad_rosenbrock, hessian_rosenbrock, x0)\n",
    "print(\"Optimal solution:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
